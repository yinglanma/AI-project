[32m[1110 20:45:40 @logger.py:59][0m Argv: ./train-atari.py --env Tutankham-v0 --gpu 2
[32m[1110 20:45:40 @param.py:144][0m Use train_log/train-atari/hyper.txt to control hyperparam learning_rate.
[32m[1110 20:45:40 @param.py:144][0m Use train_log/train-atari/hyper.txt to control hyperparam entropy_beta.
[32m[1110 20:45:40 @param.py:144][0m Use train_log/train-atari/hyper.txt to control hyperparam explore_factor.
[32m[1110 20:45:40 @train-atari.py:256][0m [BA3C] Train on gpu 0 and infer on gpu 0
[32m[1110 20:45:41 @multigpu.py:49][0m Training a model of 1 tower
[32m[1110 20:45:41 @multigpu.py:57][0m Building graph for training tower 0...
[32m[1110 20:45:41 @_common.py:61][0m conv0 input: [None, 84, 84, 12]
[32m[1110 20:45:41 @_common.py:69][0m conv0 output: [None, 84, 84, 32]
[32m[1110 20:45:41 @_common.py:61][0m pool0 input: [None, 84, 84, 32]
[32m[1110 20:45:41 @_common.py:69][0m pool0 output: [None, 42, 42, 32]
[32m[1110 20:45:41 @_common.py:61][0m conv1 input: [None, 42, 42, 32]
[32m[1110 20:45:41 @_common.py:69][0m conv1 output: [None, 42, 42, 32]
[32m[1110 20:45:41 @_common.py:61][0m pool1 input: [None, 42, 42, 32]
[32m[1110 20:45:41 @_common.py:69][0m pool1 output: [None, 21, 21, 32]
[32m[1110 20:45:41 @_common.py:61][0m conv2 input: [None, 21, 21, 32]
[32m[1110 20:45:41 @_common.py:69][0m conv2 output: [None, 21, 21, 64]
[32m[1110 20:45:41 @_common.py:61][0m pool2 input: [None, 21, 21, 64]
[32m[1110 20:45:41 @_common.py:69][0m pool2 output: [None, 10, 10, 64]
[32m[1110 20:45:41 @_common.py:61][0m conv3 input: [None, 10, 10, 64]
[32m[1110 20:45:41 @_common.py:69][0m conv3 output: [None, 10, 10, 64]
[32m[1110 20:45:41 @_common.py:61][0m fc0 input: [None, 10, 10, 64]
[32m[1110 20:45:41 @_common.py:69][0m fc0 output: [None, 512]
[32m[1110 20:45:41 @_common.py:61][0m fc-pi input: [None, 512]
[32m[1110 20:45:41 @_common.py:69][0m fc-pi output: [None, 8]
[32m[1110 20:45:41 @_common.py:61][0m fc-v input: [None, 512]
[32m[1110 20:45:41 @_common.py:69][0m fc-v output: [None, 1]
[32m[1110 20:45:42 @modelutils.py:22][0m Model Parameters: 
conv0/W:0: shape=[5, 5, 12, 32], dim=9600
conv0/b:0: shape=[32], dim=32
conv1/W:0: shape=[5, 5, 32, 32], dim=25600
conv1/b:0: shape=[32], dim=32
conv2/W:0: shape=[4, 4, 32, 64], dim=32768
conv2/b:0: shape=[64], dim=64
conv3/W:0: shape=[3, 3, 64, 64], dim=36864
conv3/b:0: shape=[64], dim=64
fc0/W:0: shape=[6400, 512], dim=3276800
fc0/b:0: shape=[512], dim=512
prelu/alpha:0: shape=[], dim=1
fc-pi/W:0: shape=[512, 8], dim=4096
fc-pi/b:0: shape=[8], dim=8
fc-v/W:0: shape=[512, 1], dim=512
fc-v/b:0: shape=[1], dim=1
Total param=3386954 (12.920204 MB assuming all float32)
[32m[1110 20:45:42 @base.py:109][0m Setup callbacks ...
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/policy_loss/EMA:0 renamed to policy_loss/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/xentropy_loss/EMA:0 renamed to xentropy_loss/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/value_loss/EMA:0 renamed to value_loss/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/predict_reward/EMA:0 renamed to predict_reward/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/rms_advantage/EMA:0 renamed to rms_advantage/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/cost/EMA:0 renamed to cost/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/policy_loss/EMA:0 renamed to policy_loss/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/xentropy_loss/EMA:0 renamed to xentropy_loss/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/value_loss/EMA:0 renamed to value_loss/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/predict_reward/EMA:0 renamed to predict_reward/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/rms_advantage/EMA:0 renamed to rms_advantage/EMA:0 when saving model.
[32m[1110 20:45:42 @common.py:57][0m [ModelSaver] tower0/cost/EMA:0 renamed to cost/EMA:0 when saving model.
[32m[1110 20:45:42 @base.py:111][0m Building graph for predictor tower 0...
[32m[1110 20:45:43 @base.py:114][0m Initializing graph variables ...
[32m[1110 20:45:44 @base.py:156][0m Starting all threads & procs ...
[32m[1110 20:45:44 @base.py:123][0m Start training with global_step=0
[32m[1110 21:21:57 @trainer.py:139][0m Enqueue Thread Exited.
